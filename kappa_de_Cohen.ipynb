{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cibelerusso/Gestao-da-qualidade/blob/main/kappa_de_Cohen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883225db",
      "metadata": {
        "id": "883225db"
      },
      "source": [
        "# Avaliação de Concordâncias entre avaliadores: Kappa de Cohen\n",
        "\n",
        "Considere um problema em que peças são inspecionada por diferentes avaliadores, que nem sempre concordam a respeito de sua conformidade.\n",
        "\n",
        "1. **Concordância Intra-avaliadores**: Consistência de um avaliador consigo mesmo em momentos distintos.\n",
        "\n",
        "2. **Concordância Entre Avaliadores**: Concordância entre dois avaliadores ao classificar os mesmos itens.\n",
        "\n",
        "Aplicação no contexto de Controle Estatístico do Processo (CEP):\n",
        "- Garantir que inspeções sejam confiáveis.\n",
        "- Identificar inconsistências entre métodos ou operadores.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ef0966",
      "metadata": {
        "id": "a2ef0966"
      },
      "source": [
        "## Coeficiente Kappa de Cohen\n",
        "\n",
        "O **coeficiente Kappa de Cohen** é uma métrica amplamente utilizada para avaliar a concordância entre dois avaliadores ou métodos de classificação, ajustando a probabilidade de concordância ao acaso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55bea888",
      "metadata": {
        "id": "55bea888"
      },
      "source": [
        "## Tabela de Contingência\n",
        "Os dados de classificação de dois avaliadores são organizados em uma matriz $n \\times n$, onde cada entrada $f_{ij}$ representa o número de observações que o Avaliador 1 atribuiu à categoria $i$ e o Avaliador 2 à categoria $j$.\n",
        "\n",
        "Por exemplo, para duas categorias (1: Aprovado, 0: Reprovado):\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <th></th>\n",
        "        <th>Avaliador 2: 1</th>\n",
        "        <th>Avaliador 2: 0</th>\n",
        "        <th>Total Avaliador 1</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Avaliador 1: 1</b></td>\n",
        "        <td>\\(f_{11}\\)</td>\n",
        "        <td>\\(f_{10}\\)</td>\n",
        "        <td>\\(n_1\\)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Avaliador 1: 0</b></td>\n",
        "        <td>\\(f_{01}\\)</td>\n",
        "        <td>\\(f_{00}\\)</td>\n",
        "        <td>\\(n_0\\)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Total</b></td>\n",
        "        <td>\\(n_1'\\)</td>\n",
        "        <td>\\(n_0'\\)</td>\n",
        "        <td>\\(N\\)</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e476b0f1",
      "metadata": {
        "id": "e476b0f1"
      },
      "source": [
        "<h2>Probabilidades</h2>\n",
        "\n",
        "<h3>Proporção Observada (<i>P<sub>o</sub></i>):</h3>\n",
        "<p>\n",
        "É a proporção de concordância real entre os dois avaliadores:\n",
        "</p>\n",
        "<p>\n",
        "$P_o = \\frac{f_{11} + f_{00}}{N}$\n",
        "</p>\n",
        "\n",
        "<h3>Probabilidade Esperada (<i>P<sub>e</sub></i>):</h3>\n",
        "<p>\n",
        "Considera a concordância esperada ao acaso, calculada com base nas margens da tabela:\n",
        "</p>\n",
        "<p>\n",
        "\\[P_e = \\frac{(n_1 \\cdot n_1') + (n_0 \\cdot n_0')}{N^2}\\]\n",
        "</p>\n",
        "\n",
        "<p>Onde:</p>\n",
        "<ul>\n",
        "    <li><b>n<sub>1</sub></b>: Total de classificações \"1\" do Avaliador 1 (\\(f_{11} + f_{10}\\)).</li>\n",
        "    <li><b>n<sub>1'</sub></b>: Total de classificações \"1\" do Avaliador 2 (\\(f_{11} + f_{01}\\)).</li>\n",
        "    <li><b>N</b>: Total de observações.</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e12087",
      "metadata": {
        "id": "07e12087"
      },
      "source": [
        "<h2>Coeficiente Kappa</h2>\n",
        "\n",
        "<p>O coeficiente Kappa é calculado como:</p>\n",
        "<p>\n",
        "$$\\kappa = \\frac{P_o - P_e}{1 - P_e}$$\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li>\\(P_o\\): Concordância observada.</li>\n",
        "    <li>\\(P_e\\): Concordância esperada ao acaso.</li>\n",
        "    <li>\\(\\kappa\\): Varia de -1 a 1.</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8bf97b6",
      "metadata": {
        "id": "b8bf97b6"
      },
      "source": [
        "### Interpretação:\n",
        "- **< 0.20**: Concordância muito fraca.\n",
        "- **0.21 - 0.40**: Concordância fraca.\n",
        "- **0.41 - 0.60**: Concordância moderada.\n",
        "- **0.61 - 0.80**: Concordância forte.\n",
        "- **0.81 - 1.00**: Concordância muito forte."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5a96c55",
      "metadata": {
        "id": "f5a96c55"
      },
      "source": [
        "##  Exemplo de Cálculo\n",
        "Dado um conjunto de classificações:\n",
        "\n",
        "Avaliador 1: [1, 1, 0, 1, 0, 1, 0, 1]  \n",
        "Avaliador 2: [1, 1, 0, 0, 0, 1, 0, 1]\n",
        "\n",
        "**Tabela de contingência**:\n",
        "\n",
        "|              | Avaliador 2: 1 | Avaliador 2: 0 | Total Avaliador 1 |\n",
        "|--------------|----------------|----------------|--------------------|\n",
        "| **Avaliador 1: 1** | 4              | 1              | 5                  |\n",
        "| **Avaliador 1: 0** | 0              | 3              | 3                  |\n",
        "| **Total**    | 4              | 4              | 8                  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c50b70",
      "metadata": {
        "id": "f9c50b70"
      },
      "source": [
        "<h3>Cálculos:</h3>\n",
        "<ol>\n",
        "    <li>\n",
        "        <b>Probabilidade Observada (<i>P<sub>o</sub></i>):</b>\n",
        "        <p>\n",
        "        \\[P_o =\\frac{f_{11} + f_{00}}{N} = \\frac{4 + 3}{8} = 0.875\n",
        "        \\]\n",
        "        </p>\n",
        "    </li>\n",
        "    <li>\n",
        "        <b>Probabilidade Esperada (<i>P<sub>e</sub></i>):</b>\n",
        "        <p>\n",
        "        \\[\n",
        "        P_e = \\frac{(n_1 \\cdot n_1') + (n_0 \\cdot n_0')}{N^2} = \\frac{(5 \\cdot 4) + (3 \\cdot 4)}{8^2} = \\frac{20+12}{64} = 5\n",
        "        \\]\n",
        "        </p>\n",
        "    </li>\n",
        "    <li>\n",
        "        <b>Coeficiente Kappa:</b>\n",
        "        <p>\n",
        "        \\[\n",
        "        \\kappa = \\frac{P_o - P_e}{1 - P_e} = \\frac{0.875 - 0.5}{1 - 0.5} = \\frac{0.375}{0.5} \\approx 0.75\n",
        "        \\]\n",
        "        </p>\n",
        "    </li>\n",
        "</ol>\n",
        "\n",
        "<h3>Interpretação:</h3>\n",
        "<p>\n",
        "Neste exemplo, \\(\\kappa = 0.75\\), indicando <b>concordância forte</b> entre os avaliadores.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40f7730",
      "metadata": {
        "id": "c40f7730"
      },
      "source": [
        "### Usando função pronta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc4a2317",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4a2317",
        "outputId": "3f4a73e3-bc53-4ca5-e291-b3497adaab5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coeficiente Kappa Interavaliadores: 0.750\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Classificações de dois avaliadores\n",
        "avaliador1 = [1, 0, 1, 1, 0, 1, 0, 1]\n",
        "avaliador2 = [1, 0, 1, 0, 0, 1, 0, 1]\n",
        "\n",
        "# Avaliar concordância entre avaliadores\n",
        "kappa_inter = cohen_kappa_score(avaliador1, avaliador2)\n",
        "print(f\"Coeficiente Kappa Interavaliadores: {kappa_inter:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8223ba5",
      "metadata": {
        "id": "a8223ba5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}